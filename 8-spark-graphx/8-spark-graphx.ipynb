{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark graph\n",
    "\n",
    "Dans les tutoriels pr√©c√©dents nous avons toujours travaill√© avec des dataset ou table.\n",
    "\n",
    "Il existe un √©cosyst√®me spark pour traiter de gros graphes via l'api **graphx** historiquement que disponible en scala il existe **graphFrames** en python.\n",
    "\n",
    "Essayons de jouer avec cette api.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©ation du context Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fdd5c3bb430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "#url par d√©faut d'une api kubernetes acc√©d√© depuis l'int√©rieur du cluster (ici le notebook tourne lui m√™me dans kubernetes)\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc:443\")\n",
    "\n",
    "#image des executors spark: pour des raisons de simplicit√© on r√©utilise l'image du notebook\n",
    "conf.set(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n",
    "\n",
    "# Nom du compte de service pour contacter l'api kubernetes : attention le package du datalab cr√©e lui m√™me cette variable d'enviromment.\n",
    "# Dans un pod du cluster kubernetes il faut lire le fichier /var/run/secrets/kubernetes.io/serviceaccount/token\n",
    "# N√©anmoins ce param√®tre est inutile car le contexte kubernetes local de ce notebook est pr√©configur√©\n",
    "# conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \n",
    "\n",
    "# Nom du namespace kubernetes\n",
    "conf.set(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n",
    "\n",
    "# Nombre d'executeur spark, il se lancera autant de pods kubernetes que le nombre indiqu√©.\n",
    "conf.set(\"spark.executor.instances\", \"10\")\n",
    "\n",
    "# M√©moire allou√© √† la JVM\n",
    "# Attention par d√©faut le pod kubernetes aura une limite sup√©rieur qui d√©pend d'autres param√®tres.\n",
    "# On manipulera plus bas pour v√©rifier la limite de m√©moire totale d'un executeur\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "\n",
    "# Param√®tres d'enregistrement des logs spark d'application\n",
    "# Attention ce param√®tres n√©cessitent la cr√©ation d'un dossier spark-history. Spark ne le fait pas lui m√™me pour des raisons obscurs\n",
    "# import s3fs\n",
    "# endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "# fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "# fs.touch('s3://tm8enk/spark-history/.keep')\n",
    "# sparkconf.set(\"spark.eventLog.enabled\",\"true\")\n",
    "# sparkconf.set(\"spark.eventLog.dir\",\"s3a://tm8enk/spark-history\")\n",
    "#ici pour g√©rer le dateTimeFormatter d√©pendant de la verion de java...\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "conf.set(\"spark.default.parallelism\",10)\n",
    "conf.set(\"spark.sql.shuffle.partitions\",10)\n",
    "conf.set(\"spark.jars.packages\",\"graphframes:graphframes:0.8.1-spark3.0-s_2.12\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note que :\n",
    "* on a pris 10 executeurs et on surcharge spark pour, lors de shuffle ou repartition, que son niveau de parallelisme soit 10 plutot que 200\n",
    "* on importe un jar au d√©marrage il va aller le chercher sur maven central, ce jar sera appell√© par la librairie python graphFrames √† travers Py4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4b2d4557-b063-420a-a465-474cfa3a9511;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.1-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.1-spark3.0-s_2.12/graphframes-0.8.1-spark3.0-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.1-spark3.0-s_2.12!graphframes.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (19ms)\n",
      ":: resolution report :: resolve 3496ms :: artifacts dl 93ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.1-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4b2d4557-b063-420a-a465-474cfa3a9511\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (281kB/8ms)\n",
      "2022-03-30 07:21:42,143 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-30 07:21:44,847 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"graph\").config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from graphframes) (1.20.3)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 154 kB 7.0 MB/s            \n",
      "\u001b[?25hInstalling collected packages: nose, graphframes\n",
      "Successfully installed graphframes-0.6 nose-1.3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constitution d'un graphe\n",
    "\n",
    "Un graphe peut √™tre vu comme 2 datasets:\n",
    "* Un **dataset de noeud** avec 2 colonnes : un identifiant et un attribut.\n",
    "* Un **dataset d'arcs** reliant 2 noeuds : avec un identifiant de noeud source, un destination et un ou des attributs sur cet arc\n",
    " \n",
    "A partir de ces 2 datasets que spark peut traiter comme des rdd √† travers le cluster, on peut travailler sur un graphe, voici un exemple simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "vertices = [(1,\"A\"), (2,\"B\"), (3, \"C\")]\n",
    "edges = [(1,2,\"love\"), (2,1,\"hate\"), (2,3,\"follow\")]\n",
    "\n",
    "v = spark.createDataFrame(vertices, [\"id\", \"name\"])\n",
    "e = spark.createDataFrame(edges, [\"src\", \"dst\", \"action\"])\n",
    "\n",
    "premierGraphe = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|src|dst|action|\n",
      "+---+---+------+\n",
      "|  1|  2|  love|\n",
      "|  2|  1|  hate|\n",
      "|  2|  3|follow|\n",
      "+---+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "premierGraphe.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faisons un graphe sur les donn√©es twitter concernant l'insee\n",
    "\n",
    "Attention les donn√©es sont aliment√©es et √† la r√©execution du notebook les √©l√©ments suivants auront peut-√™tre √©volu√©s, on peut filtrer sur la date des tweets si n√©cessaire avant le 16 avril pour retrouver le d√©roul√© ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on importe le schema\n",
    "import pickle\n",
    "schema = pickle.load( open( \"../7-spark-streaming/schema.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 07:22:20,319 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "#on lit les donnn√©es\n",
    "df = spark.read.format(\"json\")  \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"s3a://projet-spark-lab/diffusion/tweets/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de cr√©er le graph des tweets mentionnant l'insee avec comme noeud les user de twitter et comme arc, 2 utilisateurs sont reli√©s si l'√©metteur du tweet a metionn√© un autre utilisateur dans le tweet\n",
    "\n",
    "Par exemple si alice tweet \"@bob tu vas vu le dernier tweet de l'Insee\" le noeud alice sera reli√© -> √† bob.\n",
    "\n",
    "Nous avons de la chance les donn√©es de l'api twitter pr√©mache le travail.\n",
    "\n",
    "Regardons pour commencer dans le tweet de l'api twitter les donn√©es suivantes:\n",
    "* l'identififiant du tweet\n",
    "* l'identifiant de l'utilisateur qui tweete\n",
    "* son nombre de followers pour savoir s'il est influent\n",
    "* la liste des hashtags pr√©sents dans le tweet (d√©j√† donn√© par twitter)\n",
    "* une indicatrice pour savoir s'il y a des hashtag \n",
    "* le texte du tweet\n",
    "* les user mentionn√©s dans le tweet sous forme de tableau (identifiant, name, screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1380403542725365762, user_id=1229160600737132544, name='FREDERIC üá´üá∑üê∑üç¥', followers_count=1981, hashtags=['FakeNews'], has_hashtag=1, text='RT @ThierryMARIANI: Quand @MLP_officiel le d√©nonce, c‚Äôest une #FakeNews.\\nQuand l‚Äô@InseeFr le confirme, cela devient h√©las une √©vidence que‚Ä¶', user_mentions=[Row(id=87212906, id_str='87212906', indices=[3, 18], name='Thierry MARIANI', screen_name='ThierryMARIANI'), Row(id=217749896, id_str='217749896', indices=[26, 39], name='Marine Le Pen', screen_name='MLP_officiel'), Row(id=217473382, id_str='217473382', indices=[81, 89], name='Insee', screen_name='InseeFr')])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,explode,size,first\n",
    "#.select(\"id\",\"user.id\",\"user.name\",\"user.followers_count\",\"entities.hashtags\",\"text\")\n",
    "df.select(col(\"id\").alias(\"id\"),\n",
    "         col(\"user.id\").alias(\"user_id\"),\n",
    "         col(\"user.name\").alias(\"name\"),\n",
    "         col(\"user.followers_count\"),\n",
    "         col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "         size(\"entities.hashtags\").alias(\"has_hashtag\"),\n",
    "         col(\"text\"),\n",
    "         col(\"entities.user_mentions\")) \\\n",
    " .filter(col(\"has_hashtag\")>0) \\\n",
    " .head(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cr√©er le dataset des noeuds:\n",
    "* on r√©cup√©re les id et noms des √©metteurs de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertices\n",
    "tweeters=df.select(col(\"user.id\").alias(\"id\"), col(\"user.name\").alias(\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On r√©cup√®re les id et noms des user mentionn√©s (m√™me s'ils n'ont pas forc√©ment twitt√©s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_mentionned=df.select(explode(col(\"entities.user_mentions\"))).select(col(\"col.id\").alias(\"id\"),col(\"col.screen_name\").alias(\"name\")).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait l'union des 2 dataframe et on va ne retenir qu'un nom (le nom dans la mention n'est pas toujours exact au nom du compte apparemment ca √©vitera les doublons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices=tweeters.union(users_mentionned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, collect_list\n",
    "# on d√©finit un udf qui prend le premier √©l√©ment\n",
    "udf_first = udf(lambda x: x[0])\n",
    "# on groupe l'union des deux dataset par id d'utilisateur et on collect \n",
    "#sous la forme d'une liste leurs noms qui peuvent diff√©rer entre les mentions et le compte\n",
    "# on applique l'udf pour ne retenir que le nom en t√™te\n",
    "\n",
    "final_vertices=vertices.groupby(\"id\").agg(udf_first(collect_list(\"name\")).alias(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+\n",
      "|id     |name                    |\n",
      "+-------+------------------------+\n",
      "|612473 |BBCNews                 |\n",
      "|707913 |Fender                  |\n",
      "|1012981|Jean-Yves Stervinou     |\n",
      "|1162091|PierreCol               |\n",
      "|1349111|Croquignol              |\n",
      "|1536651|Rubin Sfadj             |\n",
      "|1745171|Charles                 |\n",
      "|1994321|FRANCE24                |\n",
      "|2357391|Christophe Prieuur      |\n",
      "|4478161|Adam Curtis stan account|\n",
      "+-------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_vertices.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons maintenant le dataset des arcs :\n",
    "* on prend l'identifiant de l'√©metteur\n",
    "* on prend duplique/explose la ligne pour avoir une ligne par mention dans le tweet\n",
    "* on r√©cup√®re les hashtags\n",
    "* on r√©cup√®re l'identifiant du tweet\n",
    "\n",
    "on group by user_id et mention.id (user mentionn√©) et on agggr√®ge pour avoir :\n",
    "* le nombre de tweets\n",
    "* la liste des hashtags vus entre ces 2 users dans le sens user_id -> mention.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,collect_list,flatten\n",
    "edges=df.select(col(\"user.id\").alias(\"user_id\"), \\\n",
    "               explode(col(\"entities.user_mentions\")).alias(\"mention\"),\n",
    "               col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "               \"id\") \\\n",
    " .groupby(col(\"user_id\").alias(\"src\"), \\\n",
    "          col(\"mention.id\").alias(\"dst\")) \\\n",
    " .agg(count(\"id\").alias(\"nb\"),\n",
    "      flatten(collect_list(\"hashtags\")).alias(\"hashtags\"),\n",
    "      collect_list(\"id\").alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(957 + 1) / 958]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---+--------+--------------------+\n",
      "|    src|                dst| nb|hashtags|                  id|\n",
      "+-------+-------------------+---+--------+--------------------+\n",
      "|1012981| 973130412628246529|  1|      []|[1385318639994290...|\n",
      "|1349111|          217473382|  1|      []|[1426382322706919...|\n",
      "|1536651|           17510568|  1|      []|[1382995514933850...|\n",
      "|1745171|           16649457|  1|      []|[1436253710137667...|\n",
      "|3639301|         4041503175|  1|      []|[1386964092682964...|\n",
      "|4478161|          217473382|  1|      []|[1433405191211130...|\n",
      "|4478161|         3077516146|  1|      []|[1394763627283066...|\n",
      "|5523462|          268227446|  1|      []|[1394285638040567...|\n",
      "|5891532|            5788732|  1|      []|[1394724638886875...|\n",
      "|6274062|          200659061|  1|      []|[1425104376738336...|\n",
      "|6465822|         2832878397|  1|      []|[1386338289511280...|\n",
      "|6465822|1194947441734361089|  1|      []|[1386338289511280...|\n",
      "|6504012|          393484793|  1|      []|[1384791266144231...|\n",
      "|6574682|          632913960|  1|      []|[1436401329270923...|\n",
      "|6663162|           17510568|  1|      []|[1382996047971106...|\n",
      "|6777512|           41093168|  1|      []|[1432613227507200...|\n",
      "|6812352|          304576847|  1|      []|[1430124525291769...|\n",
      "|6999492|          388870788|  1|      []|[1426563709187641...|\n",
      "|7267072|1260248609423056896|  1|      []|[1435512479963045...|\n",
      "|7400132|         1731352014|  1|      []|[1430811031463514...|\n",
      "+-------+-------------------+---+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©er le graphe\n",
    "\n",
    "Ca y est on a nos 2 structures:\n",
    "* **identifiant**, name ici seul **identifiant** est n√©cessaire name vient donner plus d'infos on pourrait ajouter d'autres colonnes\n",
    "* **identifianttwittant(src), identifiantmentionn√©(dst)**, nombre de fois, liste de hashtags, liste des ids de tweets ici seul les 2 premiers identifiants sont n√©cessaires le reste donne du contexte qu'on pourrait utiliser sur les arcs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "g = GraphFrame(final_vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a ce graphe on va le mettre en cache car nous allons le manipuler plusieurs fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: bigint, name: string], e:[src: bigint, dst: bigint ... 3 more fields])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le graphe a 46124 noeuds et 75911 arcs\n"
     ]
    }
   ],
   "source": [
    "print(\"le graphe a \"+ str(g.vertices.count()) +\" noeuds et \"+ str(g.edges.count()) + \" arcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des utilisateurs populaires\n",
    "\n",
    "On peut via l'api appliquer diff√©rents algorithmes, il est d'usage dans les graphes de traiter des notions suivantes :\n",
    "* la notion de degr√© (nombre de connexion d'un noeud)\n",
    "* la notion de triangle (triplet de noeud totalement connect√©)\n",
    "* la notion de chemin entre noeud\n",
    "\n",
    "L'algorithme pageRank est un algorithme qui est connu pour etre utilis√© dans les moteurs de recherche, il peut etre execut√© sur un graphe pour juger de la \"popularit√©\" d'un noeud.\n",
    "\n",
    "On s'attend √† ce que le comte de l'insee soit pas mal plac√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = g.pageRank(resetProbability=0.15,tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------------+------------------+\n",
      "|id                 |name                                              |pagerank          |\n",
      "+-------------------+--------------------------------------------------+------------------+\n",
      "|217473382          |Insee                                             |2006.2580214097795|\n",
      "|304576847          |maximetandonnet                                   |753.6780909654278 |\n",
      "|1312779074088194049|ùïèùüöùîªùîºùïÑüáπüá¨üá¨üá≠                                |423.54102917543133|\n",
      "|103918784          |davidlisnard                                      |372.26630218296174|\n",
      "|546941531          |Guillaume Duval                                   |346.82728313325487|\n",
      "|983334079981654016 |Docteur Peter EL BAZE                             |336.18135734295106|\n",
      "|1214315619031478272|Mediavenir                                        |335.22836144910286|\n",
      "|1499400284         |J_Bardella                                        |317.94904992322387|\n",
      "|2305392175         |superflameur                                      |229.57756545422825|\n",
      "|1281266476528386052|MoorishFacts                                      |207.01759735668492|\n",
      "|204368725          |Ian Brossat                                       |199.1568888367383 |\n",
      "|114222231          |Yves d'Am√©court                                   |194.1236884848601 |\n",
      "|374184532          |BrunoLeMaire                                      |172.59652109774296|\n",
      "|818847510546546690 |Le Discord insoumis                               |158.3744629682767 |\n",
      "|945473418          |Docteur Laurent Alexandre #JeSuisDoublementVaccin√©|149.20247470313905|\n",
      "|15618138           |coulmont                                          |147.01327165185148|\n",
      "|382736141          |Enedis                                            |141.79970487088306|\n",
      "|338985020          |Agence France-Presse                              |122.67379002355665|\n",
      "|233532563          |David Perrotin                                    |122.1835088661365 |\n",
      "|3373762791         |GWGoldnadel                                       |119.86888415432534|\n",
      "+-------------------+--------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "pagerank.vertices.sort(desc(\"pagerank\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons pourquoi David Lisnard le maire de Cannes arrive dans le top des influenceurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+--------+---------------------------------------------------------------+------+\n",
      "|src      |dst      |nb |hashtags|id                                                             |weight|\n",
      "+---------+---------+---+--------+---------------------------------------------------------------+------+\n",
      "|103918784|103918784|3  |[]      |[1380509496284430336, 1380509496284430336, 1380509496284430336]|0.5   |\n",
      "|103918784|114222231|1  |[]      |[1430429793301024771]                                          |0.5   |\n",
      "+---------+---------+---+--------+---------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank.edges.where((col(\"src\")==\"103918784\") ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagerank.edges.where((col(\"dst\")==\"103918784\") ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David lisnard n'a tweet√© qu'une fois mais il s'est fait mentionn√© 545 fois autour de ce tweet.\n",
    "\n",
    "https://twitter.com/davidlisnard/status/1380509496284430336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder les utilisateurs ayant le plus √©t√© mentionn√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------------------------------------+\n",
      "|id                 |inDegree|name                                              |\n",
      "+-------------------+--------+--------------------------------------------------+\n",
      "|217473382          |5159    |Insee                                             |\n",
      "|304576847          |2660    |maximetandonnet                                   |\n",
      "|1499400284         |1163    |J_Bardella                                        |\n",
      "|1214315619031478272|1115    |Mediavenir                                        |\n",
      "|1312779074088194049|983     |ùïèùüöùîªùîºùïÑüáπüá¨üá¨üá≠                                |\n",
      "|983334079981654016 |883     |Docteur Peter EL BAZE                             |\n",
      "|2305392175         |692     |superflameur                                      |\n",
      "|945473418          |632     |Docteur Laurent Alexandre #JeSuisDoublementVaccin√©|\n",
      "|204368725          |620     |Ian Brossat                                       |\n",
      "|978184280          |605     |DamienRieu                                        |\n",
      "+-------------------+--------+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.join(g.vertices,\"id\").orderBy(desc(\"inDegree\")).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou qui ont mentionn√© le plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------------------------------------------------+\n",
      "|id                 |outDegree|name                                              |\n",
      "+-------------------+---------+--------------------------------------------------+\n",
      "|1312779074088194049|150      |ùïèùüöùîªùîºùïÑüáπüá¨üá¨üá≠                                |\n",
      "|891710670856753152 |125      |infos etc                                         |\n",
      "|217473382          |109      |Insee                                             |\n",
      "|1107619640409276417|92       |Trinpküçì‚úä 0.1% #ResistSR                          |\n",
      "|1359816991033458689|91       |Cris Finland                                      |\n",
      "|1209500610787196929|68       |Observatoire de l'immigration et de la d√©mographie|\n",
      "|702991804661112832 |56       |Ari Kouts                                         |\n",
      "|1378687476697550852|53       |BouleBille                                        |\n",
      "|1222491562824949763|53       |Jacques                                           |\n",
      "|1419627065809190918|51       |Benedicte                                         |\n",
      "+-------------------+---------+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.outDegrees.join(g.vertices,\"id\").orderBy(desc(\"outDegree\")).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'api propose d'autres algorithmes permettant:\n",
    "* de clusteriser le graphe (connected ou strong connected components)\n",
    "* de rechercher des patterns ou chemin dans le graphe ce qui pourrait etre int√©ressant pour voir par exemple une chaine de retweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin on peut nous m√™me cr√©er nos propres algorithmes pour cela l'api propose une m√©thode aggregate messages permettant d'envoyer un message de noeud et noeud et d√©finir l'aggregation √† retenir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 08:05:36,267 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
